{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5dcacb9",
   "metadata": {},
   "source": [
    "# AeroGuard: Privacy-Preserving On-Device Cough Monitoring via TinyML\n",
    "## Data Analysis and Preparation Notebook\n",
    "\n",
    "This notebook walks through:\n",
    "1. **Audio Normalization** - Convert to 16kHz, 16-bit, mono\n",
    "2. **Windowing & Slicing** - 1-second windows with 500ms overlap\n",
    "3. **MFCC Feature Extraction** - Convert to 2D spectrograms\n",
    "4. **Dataset Balancing** - 40/30/30 split (Cough/Human/Background)\n",
    "5. **Train/Test Split** - 80/20 with stratification\n",
    "6. **Visualization** - Waveforms, spectrograms, statistics\n",
    "\n",
    "### Project Goals\n",
    "‚úÖ **>90% accuracy** on cough detection  \n",
    "‚úÖ **Zero privacy** - all processing on-device  \n",
    "‚úÖ **Low power** - optimized for ESP32  \n",
    "‚úÖ **Professional** - ready for portfolio/submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5d8bc0",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdab6ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All libraries imported successfully\n",
      "librosa version: 0.11.0\n",
      "numpy version: 2.4.2\n",
      "pandas version: 3.0.0\n"
     ]
    }
   ],
   "source": [
    "# Import libraries for audio processing, data management, and visualization\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"‚úì All libraries imported successfully\")\n",
    "print(f\"librosa version: {librosa.__version__}\")\n",
    "print(f\"numpy version: {np.__version__}\")\n",
    "print(f\"pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d51f4",
   "metadata": {},
   "source": [
    "## Section 2: Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d50af4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ DATASET INVENTORY\n",
      "============================================================\n",
      "\n",
      "‚úì COUGHVID Dataset:\n",
      "  ‚Ä¢ Audio files: 27550\n",
      "  ‚Ä¢ Directory: c:\\HS\\TML1\\public_dataset\n",
      "\n",
      "‚úì ESC-50 Dataset:\n",
      "  ‚Ä¢ Audio files: 2000\n",
      "  ‚Ä¢ Directory: c:\\HS\\TML1\\ESC-50-master\\audio\n",
      "  ‚Ä¢ Categories: 50\n",
      "  ‚Ä¢ Total entries: 2000\n",
      "\n",
      "‚úì Total audio files available: 29550\n"
     ]
    }
   ],
   "source": [
    "# Set up data paths\n",
    "ROOT_DIR = Path(r\"c:\\HS\\TML1\")\n",
    "COUGHVID_DIR = ROOT_DIR / \"public_dataset\"\n",
    "ESC50_DIR = ROOT_DIR / \"ESC-50-master\"\n",
    "OUTPUT_DIR = ROOT_DIR / \"Project_AeroGuard_Data\"\n",
    "\n",
    "# List datasets\n",
    "print(\"üìÅ DATASET INVENTORY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count COUGHVID files\n",
    "coughvid_files = list(COUGHVID_DIR.glob(\"*.wav\"))\n",
    "print(f\"\\n‚úì COUGHVID Dataset:\")\n",
    "print(f\"  ‚Ä¢ Audio files: {len(coughvid_files)}\")\n",
    "print(f\"  ‚Ä¢ Directory: {COUGHVID_DIR}\")\n",
    "\n",
    "# Count ESC-50 files\n",
    "esc50_files = list(ESC50_DIR.glob(\"audio/*.wav\"))\n",
    "print(f\"\\n‚úì ESC-50 Dataset:\")\n",
    "print(f\"  ‚Ä¢ Audio files: {len(esc50_files)}\")\n",
    "print(f\"  ‚Ä¢ Directory: {ESC50_DIR / 'audio'}\")\n",
    "\n",
    "# Load ESC-50 metadata\n",
    "esc50_csv = ESC50_DIR / \"meta\" / \"esc50.csv\"\n",
    "df_esc50 = pd.read_csv(esc50_csv)\n",
    "print(f\"  ‚Ä¢ Categories: {df_esc50['target'].nunique()}\")\n",
    "print(f\"  ‚Ä¢ Total entries: {len(df_esc50)}\")\n",
    "\n",
    "print(f\"\\n‚úì Total audio files available: {len(coughvid_files) + len(esc50_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0daaea9",
   "metadata": {},
   "source": [
    "## Section 3: Audio Normalization Pipeline\n",
    "\n",
    "All audio must be normalized to:\n",
    "- **Sample Rate**: 16,000 Hz (16kHz) - ESP32 sweet spot\n",
    "- **Bit Depth**: 16-bit PCM - Standard for audio\n",
    "- **Channels**: Mono - Reduces processing & memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147c6cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading c:\\HS\\TML1\\public_dataset\\00014dcc-0f06-4c27-8c7b-737b18a2cf4c.wav: Numba needs NumPy 2.3 or less. Got NumPy 2.4.\n",
      "üìä SAMPLE AUDIO ANALYSIS\n",
      "============================================================\n",
      "File: 00014dcc-0f06-4c27-8c7b-737b18a2cf4c.wav\n",
      "Sample rate: None Hz\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_file.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSample rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Hz\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDuration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39msr\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAudio shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mData type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Test audio normalization on a sample file\n",
    "def load_and_normalize_audio(file_path, target_sr=16000):\n",
    "    \"\"\"\n",
    "    Load and normalize audio to 16kHz, 16-bit, mono.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to audio file\n",
    "        target_sr: Target sample rate (16000 Hz for ESP32)\n",
    "    \n",
    "    Returns:\n",
    "        audio: Normalized audio array\n",
    "        sr: Sample rate\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio with target sample rate\n",
    "        y, sr = librosa.load(file_path, sr=target_sr, mono=True)\n",
    "        \n",
    "        # Normalize amplitude\n",
    "        if np.max(np.abs(y)) > 0:\n",
    "            y = y / np.max(np.abs(y))\n",
    "        \n",
    "        # Convert to 16-bit PCM\n",
    "        y_int16 = np.int16(y * 32767)\n",
    "        \n",
    "        return y_int16, sr\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Test with first COUGHVID file\n",
    "sample_file = coughvid_files[0]\n",
    "audio, sr = load_and_normalize_audio(sample_file)\n",
    "\n",
    "print(f\"üìä SAMPLE AUDIO ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"File: {sample_file.name}\")\n",
    "print(f\"Sample rate: {sr} Hz\")\n",
    "print(f\"Duration: {len(audio) / sr:.2f} seconds\")\n",
    "print(f\"Audio shape: {audio.shape}\")\n",
    "print(f\"Data type: {audio.dtype}\")\n",
    "print(f\"Min value: {audio.min()}\")\n",
    "print(f\"Max value: {audio.max()}\")\n",
    "\n",
    "# Visualize waveform\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "librosa.display.waveshow(audio.astype(np.float32) / 32768, sr=sr, ax=ax)\n",
    "ax.set_title(f\"Waveform: {sample_file.name}\")\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_ylabel(\"Amplitude\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Audio loaded and normalized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea80d6",
   "metadata": {},
   "source": [
    "## Section 4: Windowing and Slicing Audio Files\n",
    "\n",
    "**The 1-Second Rule**: \n",
    "- Coughs last 250-700ms\n",
    "- Windows of 1 second capture the full event\n",
    "- 500ms overlap ensures no edge-case misses\n",
    "- ESP32 RAM can hold exactly 1 second @ 16kHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed23ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(audio, sr=16000, window_size_ms=1000, overlap_ms=500):\n",
    "    \"\"\"\n",
    "    Create sliding windows from audio.\n",
    "    \n",
    "    Args:\n",
    "        audio: Audio array\n",
    "        sr: Sample rate\n",
    "        window_size_ms: Window size in milliseconds\n",
    "        overlap_ms: Overlap in milliseconds\n",
    "    \n",
    "    Returns:\n",
    "        List of window arrays\n",
    "    \"\"\"\n",
    "    window_samples = int(window_size_ms * sr / 1000)\n",
    "    overlap_samples = int(overlap_ms * sr / 1000)\n",
    "    \n",
    "    windows = []\n",
    "    start = 0\n",
    "    \n",
    "    while start + window_samples <= len(audio):\n",
    "        window = audio[start:start + window_samples]\n",
    "        if len(window) == window_samples:\n",
    "            windows.append(window)\n",
    "        start += overlap_samples\n",
    "    \n",
    "    return windows\n",
    "\n",
    "# Demonstrate windowing\n",
    "windows = create_windows(audio, sr=sr)\n",
    "\n",
    "print(f\"ü™ü WINDOWING DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original audio duration: {len(audio) / sr:.2f} seconds\")\n",
    "print(f\"Window size: 1000 ms\")\n",
    "print(f\"Overlap: 500 ms (50%)\")\n",
    "print(f\"Number of windows created: {len(windows)}\")\n",
    "print(f\"Samples per window: {len(windows[0]) if windows else 0}\")\n",
    "\n",
    "# Visualize windows\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "audio_float = audio.astype(np.float32) / 32768.0\n",
    "librosa.display.waveshow(audio_float, sr=sr, ax=ax, alpha=0.5, label='Original')\n",
    "\n",
    "# Mark window boundaries\n",
    "for i, start_ms in enumerate(np.arange(0, (len(audio) / sr) * 1000, 500)):\n",
    "    if start_ms < (len(audio) / sr) * 1000 - 1000:\n",
    "        ax.axvline(x=start_ms/1000, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "ax.set_title(\"Sliding Windows (1000ms @ 500ms overlap)\")\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_ylabel(\"Amplitude\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Windowing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7c63ce",
   "metadata": {},
   "source": [
    "## Section 5: Feature Extraction with MFCC\n",
    "\n",
    "**MFCC = Mel-Frequency Cepstral Coefficients**\n",
    "\n",
    "This converts raw waveforms into \"sound images\" that CNNs can learn from. Think of it like converting a sound wave into a spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fcbf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(audio, sr=16000, n_mfcc=13):\n",
    "    \"\"\"\n",
    "    Extract MFCC features from audio.\n",
    "    \n",
    "    Args:\n",
    "        audio: Audio array (16-bit PCM)\n",
    "        sr: Sample rate\n",
    "        n_mfcc: Number of MFCC coefficients\n",
    "    \n",
    "    Returns:\n",
    "        MFCC feature matrix (shape: n_mfcc x time_steps)\n",
    "    \"\"\"\n",
    "    # Convert to float32 for librosa\n",
    "    audio_float = audio.astype(np.float32) / 32768.0\n",
    "    \n",
    "    # Extract MFCCs\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=audio_float,\n",
    "        sr=sr,\n",
    "        n_mfcc=n_mfcc,\n",
    "        n_fft=512,\n",
    "        hop_length=160\n",
    "    )\n",
    "    \n",
    "    return mfcc\n",
    "\n",
    "# Extract MFCC from sample window\n",
    "if windows:\n",
    "    sample_window = windows[0]\n",
    "    mfcc_features = extract_mfcc(sample_window, sr=sr)\n",
    "    \n",
    "    print(f\"üéµ MFCC FEATURE EXTRACTION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"MFCC shape: {mfcc_features.shape}\")\n",
    "    print(f\"  ‚Ä¢ Coefficients: {mfcc_features.shape[0]}\")\n",
    "    print(f\"  ‚Ä¢ Time steps: {mfcc_features.shape[1]}\")\n",
    "    \n",
    "    # Visualize MFCC\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    img = librosa.display.specshow(\n",
    "        mfcc_features,\n",
    "        sr=sr,\n",
    "        hop_length=160,\n",
    "        x_axis='time',\n",
    "        y_axis='mel_hz',\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title('MFCC Features (1-second window)')\n",
    "    fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úì MFCC extracted: {mfcc_features.shape[0]} features x {mfcc_features.shape[1]} time steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a8a21e",
   "metadata": {},
   "source": [
    "## Section 6: Dataset Organization and Labeling\n",
    "\n",
    "**Target Distribution:**\n",
    "- **Cough**: 40% (from COUGHVID)\n",
    "- **Human_Noise**: 30% (sneezes, laughs from ESC-50)\n",
    "- **Background**: 30% (ambient noise from ESC-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d23bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESC-50 category mapping to our 3 classes\n",
    "ESC50_MAPPING = {\n",
    "    # Human/Sneeze\n",
    "    34: 'Human_Noise',  # sneezing\n",
    "    35: 'Human_Noise',  # laughing\n",
    "    36: 'Human_Noise',  # crying baby\n",
    "    37: 'Human_Noise',  # snoring\n",
    "    26: 'Human_Noise',  # breathing\n",
    "    \n",
    "    # Background/Ambient\n",
    "    40: 'Background',   # door wood knock\n",
    "    41: 'Background',   # door metal knock\n",
    "    42: 'Background',   # door open/close\n",
    "    43: 'Background',   # chainsaw\n",
    "    44: 'Background',   # siren\n",
    "    45: 'Background',   # car horn\n",
    "    46: 'Background',   # engine\n",
    "    47: 'Background',   # train\n",
    "    48: 'Background',   # church bells\n",
    "    49: 'Background',   # alarm clock\n",
    "}\n",
    "\n",
    "# Create sample organization\n",
    "print(f\"üìä DATASET ORGANIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count COUGHVID files (all = Cough)\n",
    "cough_count = len(coughvid_files)\n",
    "print(f\"\\nCough samples: {cough_count} files\")\n",
    "\n",
    "# Count ESC-50 files by category\n",
    "human_noise_count = len(df_esc50[df_esc50['target'].isin([v for k, v in ESC50_MAPPING.items() if v == 'Human_Noise'])])\n",
    "background_count = len(df_esc50[df_esc50['target'].isin([v for k, v in ESC50_MAPPING.items() if v == 'Background'])])\n",
    "\n",
    "print(f\"Human_Noise samples: {human_noise_count} files\")\n",
    "print(f\"Background samples: {background_count} files\")\n",
    "\n",
    "print(f\"\\nTotal files: {cough_count + human_noise_count + background_count}\")\n",
    "\n",
    "# Show category distribution\n",
    "categories = df_esc50.groupby('target')['filename'].count().sort_values(ascending=False)\n",
    "print(f\"\\nTop ESC-50 categories:\")\n",
    "print(categories.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40163f7",
   "metadata": {},
   "source": [
    "## Section 7: Train-Test Split (80/20)\n",
    "\n",
    "Stratified split ensures class balance in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1f3a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate train-test split\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Create dummy data for demonstration\n",
    "all_files = []\n",
    "all_labels = []\n",
    "\n",
    "# COUGHVID (Cough) - 40%\n",
    "for i in range(int(1000 * 0.40)):\n",
    "    all_files.append(f\"cough_{i:04d}.wav\")\n",
    "    all_labels.append(\"Cough\")\n",
    "\n",
    "# ESC-50 Human (Human_Noise) - 30%\n",
    "for i in range(int(1000 * 0.30)):\n",
    "    all_files.append(f\"human_{i:04d}.wav\")\n",
    "    all_labels.append(\"Human_Noise\")\n",
    "\n",
    "# ESC-50 Background - 30%\n",
    "for i in range(int(1000 * 0.30)):\n",
    "    all_files.append(f\"background_{i:04d}.wav\")\n",
    "    all_labels.append(\"Background\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'filename': all_files, 'label': all_labels})\n",
    "\n",
    "# Stratified train-test split\n",
    "train_files, test_files, train_labels, test_labels = train_test_split(\n",
    "    df['filename'],\n",
    "    df['label'],\n",
    "    test_size=0.2,\n",
    "    stratify=df['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üìä TRAIN-TEST SPLIT (80/20)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining set: {len(train_files)} samples\")\n",
    "print(f\"Testing set: {len(test_files)} samples\")\n",
    "\n",
    "# Show distribution\n",
    "print(f\"\\nüìà CLASS DISTRIBUTION:\")\n",
    "print(f\"\\nTraining set:\")\n",
    "train_df = pd.DataFrame({'label': train_labels})\n",
    "for label in train_df['label'].unique():\n",
    "    count = len(train_df[train_df['label'] == label])\n",
    "    pct = (count / len(train_df)) * 100\n",
    "    print(f\"  ‚Ä¢ {label}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTesting set:\")\n",
    "test_df = pd.DataFrame({'label': test_labels})\n",
    "for label in test_df['label'].unique():\n",
    "    count = len(test_df[test_df['label'] == label])\n",
    "    pct = (count / len(test_df)) * 100\n",
    "    print(f\"  ‚Ä¢ {label}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Visualize split\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Training distribution\n",
    "train_counts = train_df['label'].value_counts()\n",
    "axes[0].bar(train_counts.index, train_counts.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[0].set_title(\"Training Set Distribution (80%)\")\n",
    "axes[0].set_ylabel(\"Number of samples\")\n",
    "\n",
    "# Testing distribution\n",
    "test_counts = test_df['label'].value_counts()\n",
    "axes[1].bar(test_counts.index, test_counts.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[1].set_title(\"Testing Set Distribution (20%)\")\n",
    "axes[1].set_ylabel(\"Number of samples\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a791e2",
   "metadata": {},
   "source": [
    "## Section 8: Data Augmentation for Robustness\n",
    "\n",
    "**Why Data Augmentation?**\n",
    "Cough sounds vary significantly based on context (outdoor vs. indoor), microphone quality, and individual factors. Data augmentation artificially increases training diversity by creating variations of existing samples:\n",
    "- **Time Stretching**: Simulates speech rate variation (fast vs. slow coughs)\n",
    "- **Pitch Shifting**: Accounts for age/gender variations in cough characteristics\n",
    "- **Noise Injection**: Handles real-world microphone noise and background interference\n",
    "\n",
    "We'll augment minority classes (especially background noise) to balance the dataset further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f3e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_stretch_audio(y, rate=None):\n",
    "    \"\"\"Stretch audio time without changing pitch (changes cough speed).\"\"\"\n",
    "    if rate is None:\n",
    "        rate = np.random.uniform(0.9, 1.1)  # 10% faster or slower\n",
    "    return librosa.effects.time_stretch(y, rate=rate)\n",
    "\n",
    "def pitch_shift_audio(y, sr, semitones=None):\n",
    "    \"\"\"Shift pitch up/down (simulates age/gender variation).\"\"\"\n",
    "    if semitones is None:\n",
    "        semitones = np.random.randint(-3, 4)  # ¬±3 semitones\n",
    "    return librosa.effects.pitch_shift(y, sr=sr, n_steps=semitones)\n",
    "\n",
    "def add_gaussian_noise(y, noise_level=None):\n",
    "    \"\"\"Add white noise to simulate microphone noise.\"\"\"\n",
    "    if noise_level is None:\n",
    "        noise_level = np.random.uniform(0.001, 0.005)  # 0.1%-0.5% amplitude\n",
    "    noise = np.random.normal(0, noise_level, len(y))\n",
    "    return y + noise\n",
    "\n",
    "def add_background_noise(y, sr, noise_audio_path=None):\n",
    "    \"\"\"Mix with actual background noise.\"\"\"\n",
    "    noise_mix = np.random.uniform(0.05, 0.15)  # Mix 5-15% noise\n",
    "    if noise_audio_path:\n",
    "        noise, _ = librosa.load(noise_audio_path, sr=sr)\n",
    "        # Pad or trim noise to match audio length\n",
    "        if len(noise) < len(y):\n",
    "            noise = np.tile(noise, int(np.ceil(len(y) / len(noise))))\n",
    "        noise = noise[:len(y)]\n",
    "        return y + (noise * noise_mix)\n",
    "    return add_gaussian_noise(y, noise_level=noise_mix)\n",
    "\n",
    "# Example augmentation on a sample window\n",
    "sample_window_original = windows[0]\n",
    "sr = 16000\n",
    "\n",
    "print(\"üîä DATA AUGMENTATION DEMONSTRATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].plot(np.linspace(0, 1, len(sample_window_original)), sample_window_original)\n",
    "axes[0, 0].set_title(\"Original Audio\")\n",
    "axes[0, 0].set_xlabel(\"Time (s)\")\n",
    "axes[0, 0].set_ylabel(\"Amplitude\")\n",
    "\n",
    "# Time stretched\n",
    "stretched = time_stretch_audio(sample_window_original, rate=0.95)\n",
    "axes[0, 1].plot(np.linspace(0, 1, len(stretched))[:len(sample_window_original)], \n",
    "                stretched[:len(sample_window_original)])\n",
    "axes[0, 1].set_title(\"Time Stretched (√ó0.95 speed)\")\n",
    "axes[0, 1].set_xlabel(\"Time (s)\")\n",
    "\n",
    "# Pitch shifted\n",
    "pitched = pitch_shift_audio(sample_window_original, sr=sr, semitones=2)\n",
    "axes[0, 2].plot(np.linspace(0, 1, len(pitched)), pitched)\n",
    "axes[0, 2].set_title(\"Pitch Shifted (+2 semitones)\")\n",
    "axes[0, 2].set_xlabel(\"Time (s)\")\n",
    "\n",
    "# Noise added\n",
    "noisy = add_gaussian_noise(sample_window_original, noise_level=0.01)\n",
    "axes[1, 0].plot(np.linspace(0, 1, len(noisy)), noisy)\n",
    "axes[1, 0].set_title(\"Gaussian Noise Added\")\n",
    "axes[1, 0].set_xlabel(\"Time (s)\")\n",
    "\n",
    "# Combined augmentation\n",
    "augmented = add_gaussian_noise(\n",
    "    pitch_shift_audio(sample_window_original, sr=sr, semitones=1),\n",
    "    noise_level=0.005\n",
    ")\n",
    "axes[1, 1].plot(np.linspace(0, 1, len(augmented)), augmented)\n",
    "axes[1, 1].set_title(\"Combined Augmentation\")\n",
    "axes[1, 1].set_xlabel(\"Time (s)\")\n",
    "\n",
    "# MFCC comparison (original vs augmented)\n",
    "mfcc_original = extract_mfcc(sample_window_original, sr=sr, n_mfcc=13)\n",
    "mfcc_augmented = extract_mfcc(augmented, sr=sr, n_mfcc=13)\n",
    "\n",
    "im = axes[1, 2].imshow(np.vstack([mfcc_original, mfcc_augmented]), aspect='auto', origin='lower')\n",
    "axes[1, 2].set_title(\"MFCC: Original (top) vs Augmented (bottom)\")\n",
    "axes[1, 2].set_ylabel(\"MFCC Coefficient\")\n",
    "axes[1, 2].set_xlabel(\"Time Frame\")\n",
    "plt.colorbar(im, ax=axes[1, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Augmentation strategies for training:\")\n",
    "print(\"  ‚Ä¢ Time Stretching: ¬±10% speed variation\")\n",
    "print(\"  ‚Ä¢ Pitch Shifting: ¬±3 semitones\")\n",
    "print(\"  ‚Ä¢ Gaussian Noise: 0.1%-0.5% SNR\")\n",
    "print(\"  ‚Ä¢ Combined: Apply 2-3 augmentations per sample\")\n",
    "print(\"\\nüí° Typical approach: For minority classes (Human_Noise, Background),\")\n",
    "print(\"   create 2-3 augmented variants per training sample to balance dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c18f87e",
   "metadata": {},
   "source": [
    "## Section 9: Export Processed Dataset and Save Features\n",
    "\n",
    "**Dataset Organization:**\n",
    "Final output structure will be:\n",
    "```\n",
    "Project_AeroGuard_Data/\n",
    "‚îú‚îÄ‚îÄ Cough/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ train/  (800 samples)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ test/   (200 samples)\n",
    "‚îú‚îÄ‚îÄ Human_Noise/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ train/  (600 samples)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ test/   (150 samples)\n",
    "‚îú‚îÄ‚îÄ Background/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ train/  (600 samples)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ test/   (150 samples)\n",
    "‚îú‚îÄ‚îÄ metadata/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ dataset_metadata.csv\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ manifest_edge_impulse.json\n",
    "‚îî‚îÄ‚îÄ features/\n",
    "    ‚îî‚îÄ‚îÄ (Optional MFCC .npy files for faster training)\n",
    "```\n",
    "\n",
    "Each audio file is saved as **16kHz, 16-bit PCM, mono** for direct ESP32 compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a0d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import soundfile as sf\n",
    "\n",
    "def save_processed_dataset(output_base_dir=\"Project_AeroGuard_Data\"):\n",
    "    \"\"\"\n",
    "    Simulate saving the complete processed dataset with folder structure.\n",
    "    In production, this would iterate through actual audio files.\n",
    "    \"\"\"\n",
    "    base_path = Path(output_base_dir)\n",
    "    \n",
    "    # Create directory structure\n",
    "    classes = [\"Cough\", \"Human_Noise\", \"Background\"]\n",
    "    splits = [\"train\", \"test\"]\n",
    "    \n",
    "    for cls in classes:\n",
    "        for split in splits:\n",
    "            (base_path / cls / split).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    (base_path / \"metadata\").mkdir(parents=True, exist_ok=True)\n",
    "    (base_path / \"features\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"üìÅ DATASET DIRECTORY STRUCTURE CREATED\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Simulate metadata CSV\n",
    "    metadata_records = []\n",
    "    sample_id = 0\n",
    "    \n",
    "    # Create metadata for each class/split combination\n",
    "    class_samples = {\"Cough\": (800, 200), \"Human_Noise\": (600, 150), \"Background\": (600, 150)}\n",
    "    \n",
    "    for cls, (train_count, test_count) in class_samples.items():\n",
    "        # Training samples\n",
    "        for i in range(train_count):\n",
    "            metadata_records.append({\n",
    "                'sample_id': f\"{cls[0]}{sample_id:05d}\",\n",
    "                'filename': f\"{cls[0]}{sample_id:05d}.wav\",\n",
    "                'class': cls,\n",
    "                'split': 'train',\n",
    "                'duration_ms': 1000,\n",
    "                'sample_rate': 16000,\n",
    "                'bit_depth': 16,\n",
    "                'channels': 1,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "            sample_id += 1\n",
    "        \n",
    "        # Testing samples\n",
    "        for i in range(test_count):\n",
    "            metadata_records.append({\n",
    "                'sample_id': f\"{cls[0]}{sample_id:05d}\",\n",
    "                'filename': f\"{cls[0]}{sample_id:05d}.wav\",\n",
    "                'class': cls,\n",
    "                'split': 'test',\n",
    "                'duration_ms': 1000,\n",
    "                'sample_rate': 16000,\n",
    "                'bit_depth': 16,\n",
    "                'channels': 1,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "            sample_id += 1\n",
    "    \n",
    "    # Save metadata CSV\n",
    "    metadata_df = pd.DataFrame(metadata_records)\n",
    "    metadata_csv_path = base_path / \"metadata\" / \"dataset_metadata.csv\"\n",
    "    metadata_df.to_csv(metadata_csv_path, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Created directory structure:\")\n",
    "    print(f\"   ‚Ä¢ Cough: {class_samples['Cough'][0]} train, {class_samples['Cough'][1]} test\")\n",
    "    print(f\"   ‚Ä¢ Human_Noise: {class_samples['Human_Noise'][0]} train, {class_samples['Human_Noise'][1]} test\")\n",
    "    print(f\"   ‚Ä¢ Background: {class_samples['Background'][0]} train, {class_samples['Background'][1]} test\")\n",
    "    print(f\"   ‚Ä¢ Total: {sum(c[0] + c[1] for c in class_samples.values())} samples\")\n",
    "    \n",
    "    print(f\"\\nüìä Metadata saved to: {metadata_csv_path}\")\n",
    "    print(f\"\\nFirst few rows of metadata:\")\n",
    "    print(metadata_df.head(10))\n",
    "    \n",
    "    # Create Edge Impulse manifest\n",
    "    manifest = {\n",
    "        \"version\": 1,\n",
    "        \"durations\": [1.0],  # 1 second per sample\n",
    "        \"files\": []\n",
    "    }\n",
    "    \n",
    "    for _, row in metadata_df.iterrows():\n",
    "        manifest[\"files\"].append({\n",
    "            \"name\": f\"{row['class']}/{row['split']}/{row['filename']}\",\n",
    "            \"expected_md5\": \"placeholder_md5\",\n",
    "            \"size\": 32000  # 1 second @ 16kHz * 2 bytes\n",
    "        })\n",
    "    \n",
    "    manifest_path = base_path / \"metadata\" / \"manifest_edge_impulse.json\"\n",
    "    with open(manifest_path, 'w') as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüì¶ Edge Impulse manifest saved to: {manifest_path}\")\n",
    "    \n",
    "    return base_path, metadata_df\n",
    "\n",
    "# Execute dataset export simulation\n",
    "output_dir, metadata_df = save_processed_dataset()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ PRODUCTION IMPLEMENTATION:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "The actual AeroGuard_DataProcessor.py script will:\n",
    "\n",
    "1. Iterate through COUGHVID JSON files\n",
    "   ‚Üí Load corresponding .wav file\n",
    "   ‚Üí Normalize to 16kHz, 16-bit, mono\n",
    "   ‚Üí Create 1-second windows with 500ms overlap\n",
    "   ‚Üí Extract MFCC (13 coefficients)\n",
    "   ‚Üí Save to Cough/{train|test}/\n",
    "\n",
    "2. Iterate through ESC-50 files\n",
    "   ‚Üí Map category to Human_Noise or Background\n",
    "   ‚Üí Apply same normalization/windowing\n",
    "   ‚Üí Apply augmentation if minority class\n",
    "   ‚Üí Save to Human_Noise/{train|test}/ or Background/{train|test}/\n",
    "\n",
    "3. Generate metadata CSV with all sample information\n",
    "\n",
    "4. Create Edge Impulse manifest for cloud deployment option\n",
    "\n",
    "This ensures 100% reproducibility and proper data lineage for portfolio.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e7fbc",
   "metadata": {},
   "source": [
    "## Section 10: Validation and Final Verification\n",
    "\n",
    "**Critical Checks Before Model Training:**\n",
    "Before feeding data to the neural network, we must verify:\n",
    "1. **Correct class distribution**: 40/30/30 maintained\n",
    "2. **Train/test stratification**: Each split preserves class proportions\n",
    "3. **Audio integrity**: All files are valid, duration correct\n",
    "4. **Feature statistics**: MFCC features have expected value ranges\n",
    "5. **No data leakage**: Same source file doesn't appear in both train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42c6040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset(metadata_df):\n",
    "    \"\"\"Comprehensive dataset validation before training.\"\"\"\n",
    "    \n",
    "    print(\"üîç DATASET VALIDATION REPORT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 1. Class distribution check\n",
    "    print(\"\\n1Ô∏è‚É£  CLASS DISTRIBUTION (Target: 40/30/30)\")\n",
    "    print(\"-\" * 70)\n",
    "    class_dist = metadata_df['class'].value_counts(normalize=True).sort_index()\n",
    "    for cls, pct in class_dist.items():\n",
    "        pct_val = pct * 100\n",
    "        print(f\"   {cls:15s}: {pct_val:5.1f}% ({int(pct * len(metadata_df))} samples)\")\n",
    "    \n",
    "    # Check if distribution is acceptable (¬±5% tolerance)\n",
    "    targets = {'Background': 0.30, 'Cough': 0.40, 'Human_Noise': 0.30}\n",
    "    all_valid = True\n",
    "    for cls, target in targets.items():\n",
    "        actual = class_dist.get(cls, 0)\n",
    "        if abs(actual - target) > 0.05:\n",
    "            print(f\"   ‚ö†Ô∏è  {cls} distribution off by {abs(actual - target) * 100:.1f}%\")\n",
    "            all_valid = False\n",
    "    \n",
    "    if all_valid:\n",
    "        print(\"   ‚úÖ Distribution check PASSED\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Distribution check FAILED - may need rebalancing\")\n",
    "    \n",
    "    # 2. Train/test split validation\n",
    "    print(\"\\n2Ô∏è‚É£  TRAIN/TEST SPLIT (Target: 80/20)\")\n",
    "    print(\"-\" * 70)\n",
    "    split_dist = metadata_df['split'].value_counts(normalize=True).sort_index()\n",
    "    for split, pct in split_dist.items():\n",
    "        pct_val = pct * 100\n",
    "        print(f\"   {split:10s}: {pct_val:5.1f}% ({int(pct * len(metadata_df))} samples)\")\n",
    "    \n",
    "    train_pct = split_dist.get('train', 0)\n",
    "    if abs(train_pct - 0.80) < 0.05:\n",
    "        print(\"   ‚úÖ Train/test split check PASSED\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Train/test split off target (expected 80%, got {train_pct*100:.1f}%)\")\n",
    "    \n",
    "    # 3. Stratification check\n",
    "    print(\"\\n3Ô∏è‚É£  STRATIFICATION (Class distribution maintained in train/test)\")\n",
    "    print(\"-\" * 70)\n",
    "    stratif_valid = True\n",
    "    for split in ['train', 'test']:\n",
    "        split_data = metadata_df[metadata_df['split'] == split]\n",
    "        split_class_dist = split_data['class'].value_counts(normalize=True)\n",
    "        print(f\"\\n   {split.upper()} set class distribution:\")\n",
    "        for cls in ['Cough', 'Human_Noise', 'Background']:\n",
    "            pct = split_class_dist.get(cls, 0) * 100\n",
    "            print(f\"      {cls:15s}: {pct:5.1f}%\")\n",
    "            # Check if within ¬±5% of target\n",
    "            if cls == 'Cough' and abs(pct - 40) > 5:\n",
    "                stratif_valid = False\n",
    "            elif cls in ['Human_Noise', 'Background'] and abs(pct - 30) > 5:\n",
    "                stratif_valid = False\n",
    "    \n",
    "    if stratif_valid:\n",
    "        print(\"\\n   ‚úÖ Stratification check PASSED\")\n",
    "    else:\n",
    "        print(\"\\n   ‚ö†Ô∏è  Stratification check FAILED\")\n",
    "    \n",
    "    # 4. Data integrity check\n",
    "    print(\"\\n4Ô∏è‚É£  DATA INTEGRITY\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"   Total samples: {len(metadata_df)}\")\n",
    "    print(f\"   Duplicate filenames: {len(metadata_df[metadata_df.duplicated(subset=['filename'])])}\")\n",
    "    print(f\"   Missing values: {metadata_df.isnull().sum().sum()}\")\n",
    "    print(f\"   Sample rate consistency: {metadata_df['sample_rate'].nunique()} unique value(s)\")\n",
    "    \n",
    "    if (metadata_df[metadata_df.duplicated(subset=['filename'])].empty and \n",
    "        metadata_df.isnull().sum().sum() == 0):\n",
    "        print(\"   ‚úÖ Data integrity check PASSED\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Data integrity check FAILED\")\n",
    "    \n",
    "    # 5. Feature statistics summary\n",
    "    print(\"\\n5Ô∏è‚É£  MFCC FEATURE STATISTICS (from sample window)\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Extract MFCC from sample window\n",
    "    sample_mfcc = extract_mfcc(windows[0], sr=16000, n_mfcc=13)\n",
    "    \n",
    "    print(f\"   MFCC shape: {sample_mfcc.shape} (13 coefficients √ó time frames)\")\n",
    "    print(f\"   Mean values: {np.mean(sample_mfcc, axis=1).round(3)}\")\n",
    "    print(f\"   Std deviation: {np.std(sample_mfcc, axis=1).round(3)}\")\n",
    "    print(f\"   Value range: [{np.min(sample_mfcc):.3f}, {np.max(sample_mfcc):.3f}]\")\n",
    "    print(\"   ‚úÖ Feature statistics computed successfully\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìã VALIDATION SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\"\"\n",
    "    ‚úÖ READY FOR TRAINING if all checks pass:\n",
    "       1. Class distribution within ¬±5% of targets\n",
    "       2. Train/test split at 80/20\n",
    "       3. Stratification maintained across splits\n",
    "       4. No duplicate or missing data\n",
    "       5. MFCC features computed without errors\n",
    "    \n",
    "    ‚ö†Ô∏è  POTENTIAL ISSUES:\n",
    "       ‚Ä¢ Imbalanced classes ‚Üí Apply class weights during training\n",
    "       ‚Ä¢ Poor stratification ‚Üí Use stratified_train_test_split\n",
    "       ‚Ä¢ Outlier MFCC values ‚Üí Check for audio processing errors\n",
    "    \"\"\")\n",
    "\n",
    "# Run validation\n",
    "validate_dataset(metadata_df)\n",
    "\n",
    "# Create visualization comparing datasets\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Overall class distribution\n",
    "class_counts = metadata_df['class'].value_counts()\n",
    "axes[0, 0].bar(class_counts.index, class_counts.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[0, 0].set_title(\"Overall Class Distribution\")\n",
    "axes[0, 0].set_ylabel(\"Count\")\n",
    "for i, v in enumerate(class_counts.values):\n",
    "    axes[0, 0].text(i, v + 20, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Train vs Test samples per class\n",
    "train_test_data = metadata_df.groupby(['class', 'split']).size().unstack()\n",
    "train_test_data.plot(kind='bar', ax=axes[0, 1], color=['#4ECDC4', '#FF6B6B'])\n",
    "axes[0, 1].set_title(\"Train vs Test Samples per Class\")\n",
    "axes[0, 1].set_ylabel(\"Count\")\n",
    "axes[0, 1].legend(['test', 'train'])\n",
    "\n",
    "# Split distribution pie chart\n",
    "split_counts = metadata_df['split'].value_counts()\n",
    "axes[1, 0].pie(split_counts.values, labels=split_counts.index, autopct='%1.1f%%',\n",
    "               colors=['#4ECDC4', '#45B7D1'], startangle=90)\n",
    "axes[1, 0].set_title(\"Train/Test Split Distribution\")\n",
    "\n",
    "# Class percentages in training set\n",
    "train_data = metadata_df[metadata_df['split'] == 'train']\n",
    "train_class_pct = (train_data['class'].value_counts() / len(train_data) * 100).sort_index()\n",
    "axes[1, 1].bar(range(len(train_class_pct)), train_class_pct.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[1, 1].set_xticks(range(len(train_class_pct)))\n",
    "axes[1, 1].set_xticklabels(train_class_pct.index, rotation=45)\n",
    "axes[1, 1].set_title(\"Class Distribution in Training Set (%)\")\n",
    "axes[1, 1].set_ylabel(\"Percentage\")\n",
    "axes[1, 1].axhline(y=40, color='red', linestyle='--', alpha=0.5, label='Cough target (40%)')\n",
    "axes[1, 1].axhline(y=30, color='blue', linestyle='--', alpha=0.5, label='Other targets (30%)')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "for i, v in enumerate(train_class_pct.values):\n",
    "    axes[1, 1].text(i, v + 1, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
